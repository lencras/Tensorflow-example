{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATADIR = r\"E:\\IDM downloads\\Documents\\Uni\\SIT University\\FYP\\ALLaaft\\aaft70\"\n",
    "CATEGORIES = ['1', '2', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2517/2517 [00:03<00:00, 809.39it/s]\n",
      "100%|██████████| 2657/2657 [00:03<00:00, 846.13it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 860.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7341\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "\n",
    "def create_sequences():\n",
    "    for category in CATEGORIES:  \n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to categories\n",
    "        class_num = CATEGORIES.index(category)  # get the classification \n",
    "\n",
    "        for csv in tqdm(os.listdir(path)):  # iterate over each csv file per category\n",
    "            try:\n",
    "                traindata = pd.read_csv(os.path.join(path,csv), header=None, names=['x', 'y', 'z'])\n",
    "                values = traindata.values\n",
    "                sequences.append((values,class_num))  # add this to sequences\n",
    "                \n",
    "            except Exception as e: \n",
    "                pass\n",
    "            except OSError as e:\n",
    "                print(\"OSError Bad csv most likely\", e, os.path.join(path,csv))\n",
    "            except Exception as e:\n",
    "                print(\"general exception\", e, os.path.join(path,csv))\n",
    "\n",
    "create_sequences()\n",
    "\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sequences)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for features,label in sequences:\n",
    "    x.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7341.000000\n",
       "mean       91.684920\n",
       "std        56.610559\n",
       "min        35.000000\n",
       "25%        57.000000\n",
       "50%        69.000000\n",
       "75%        90.000000\n",
       "max       233.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_x = []\n",
    "for one_seq in x:\n",
    "    len_x.append(len(one_seq))\n",
    "pd.Series(len_x).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Padding the sequence with the values in last row to max length\n",
    "to_pad = 233 # follow max\n",
    "new_seq = []\n",
    "for one_seq in x:\n",
    "    len_one_seq = len(one_seq)\n",
    "    last_val = one_seq[-1]\n",
    "    n = to_pad - len_one_seq\n",
    "   \n",
    "    to_concat = np.repeat(one_seq[-1], n).reshape(3, n).transpose()\n",
    "    new_one_seq = np.concatenate([one_seq, to_concat])\n",
    "    new_seq.append(new_one_seq)\n",
    "X = np.stack(new_seq)\n",
    "\n",
    "#truncate the sequence to length xx\n",
    "from keras.preprocessing import sequence\n",
    "seq_len = 91 #follow mean, either round up or down to nearest int\n",
    "X=sequence.pad_sequences(X, maxlen=seq_len, padding='post', dtype='float', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X_LSTM70.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y_LSTM70.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0330 23:13:44.951806  2392 deprecation.py:506] From C:\\Users\\admin\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-LSTM-128-nodes-1-dense-1585581224\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 91, 128)           68096     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 213,395\n",
      "Trainable params: 213,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6606 samples, validate on 735 samples\n",
      "Epoch 1/20\n",
      "6606/6606 [==============================] - 6s 980us/sample - loss: 0.5891 - acc: 0.7365 - val_loss: 0.6753 - val_acc: 0.7442\n",
      "Epoch 2/20\n",
      "6606/6606 [==============================] - 5s 765us/sample - loss: 0.4249 - acc: 0.8123 - val_loss: 0.3846 - val_acc: 0.8218\n",
      "Epoch 3/20\n",
      "6606/6606 [==============================] - 5s 752us/sample - loss: 0.3217 - acc: 0.8642 - val_loss: 0.2846 - val_acc: 0.8776\n",
      "Epoch 4/20\n",
      "6606/6606 [==============================] - 5s 759us/sample - loss: 0.2292 - acc: 0.9043 - val_loss: 0.2723 - val_acc: 0.8816\n",
      "Epoch 5/20\n",
      "6606/6606 [==============================] - 5s 752us/sample - loss: 0.1949 - acc: 0.9208 - val_loss: 0.1785 - val_acc: 0.9265\n",
      "Epoch 6/20\n",
      "6606/6606 [==============================] - 5s 779us/sample - loss: 0.1550 - acc: 0.9355 - val_loss: 0.1619 - val_acc: 0.9415\n",
      "Epoch 7/20\n",
      "6606/6606 [==============================] - 5s 757us/sample - loss: 0.1335 - acc: 0.9487 - val_loss: 0.1255 - val_acc: 0.9469\n",
      "Epoch 8/20\n",
      "6606/6606 [==============================] - 5s 752us/sample - loss: 0.1291 - acc: 0.9505 - val_loss: 0.1125 - val_acc: 0.9605\n",
      "Epoch 9/20\n",
      "6606/6606 [==============================] - 5s 756us/sample - loss: 0.0898 - acc: 0.9682 - val_loss: 0.0654 - val_acc: 0.9782\n",
      "Epoch 10/20\n",
      "6606/6606 [==============================] - 5s 756us/sample - loss: 0.1008 - acc: 0.9643 - val_loss: 0.1040 - val_acc: 0.9565\n",
      "Epoch 11/20\n",
      "6606/6606 [==============================] - 5s 760us/sample - loss: 0.0751 - acc: 0.9728 - val_loss: 0.0646 - val_acc: 0.9741\n",
      "Epoch 12/20\n",
      "6606/6606 [==============================] - 5s 754us/sample - loss: 0.0567 - acc: 0.9768 - val_loss: 0.0978 - val_acc: 0.9633\n",
      "Epoch 13/20\n",
      "6606/6606 [==============================] - 5s 752us/sample - loss: 0.0623 - acc: 0.9802 - val_loss: 0.0550 - val_acc: 0.9796\n",
      "Epoch 14/20\n",
      "6606/6606 [==============================] - 5s 759us/sample - loss: 0.0457 - acc: 0.9837 - val_loss: 0.0535 - val_acc: 0.9837\n",
      "Epoch 15/20\n",
      "6606/6606 [==============================] - 5s 754us/sample - loss: 0.0510 - acc: 0.9818 - val_loss: 0.0298 - val_acc: 0.9905\n",
      "Epoch 16/20\n",
      "6606/6606 [==============================] - 5s 751us/sample - loss: 0.0508 - acc: 0.9841 - val_loss: 0.0376 - val_acc: 0.9878\n",
      "Epoch 17/20\n",
      "6606/6606 [==============================] - 5s 755us/sample - loss: 0.0449 - acc: 0.9858 - val_loss: 0.0656 - val_acc: 0.9687\n",
      "Epoch 18/20\n",
      "6606/6606 [==============================] - 5s 750us/sample - loss: 0.0440 - acc: 0.9849 - val_loss: 0.0733 - val_acc: 0.9782\n",
      "Epoch 19/20\n",
      "6606/6606 [==============================] - 5s 751us/sample - loss: 0.0232 - acc: 0.9920 - val_loss: 0.0327 - val_acc: 0.9864\n",
      "Epoch 20/20\n",
      "6606/6606 [==============================] - 5s 756us/sample - loss: 0.0376 - acc: 0.9876 - val_loss: 0.0907 - val_acc: 0.9796\n",
      "Run the command line on anaconda prompt:\n",
      "--> tensorboard --logdir=/tmp/logs/ --host localhost --port 8088 \n",
      "Then open http://localhost:8088/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "#breakpoint()\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Embedding\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D ,LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "pickle_in = open(\"X_LSTM70.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_LSTM70.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "seq_len = 91\n",
    "X = X *1.0/100 - 1.0 \n",
    "\n",
    "dense_layers = [1]\n",
    "layer_sizes = [128]\n",
    "LSTM_layers = [2]\n",
    "\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for LSTM_layer in LSTM_layers:\n",
    "            NAME = \"{}-LSTM-{}-nodes-{}-dense-{}\".format(LSTM_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "            \n",
    "            model.add(CuDNNLSTM(layer_size, input_shape=X.shape[1:], return_sequences=True))\n",
    "            \n",
    "            \n",
    "            for a in range(LSTM_layer-2):\n",
    "                model.add(CuDNNLSTM(layer_size, input_shape=X.shape[1:], return_sequences=True))\n",
    "            \n",
    "            model.add(CuDNNLSTM(layer_size, input_shape=(X.shape[1:], 3)))\n",
    "            \n",
    "            for b in range(dense_layer):\n",
    "                model.add(Dense(100, activation='relu'))\n",
    "                model.add(Dropout(0.1))\n",
    "            \n",
    "            \n",
    "            model.add(Dense(3, activation='softmax'))\n",
    "            \n",
    "            tensorboard = TensorBoard(log_dir=\"C:\\\\tmp\\\\logs\\\\example\\\\{}\".format(NAME))\n",
    "            #earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "            #adam = tf.keras.optimizers.Adam(lr=0.01)\n",
    "            \n",
    "            model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "            \n",
    "            print(model.summary())\n",
    "            \n",
    "            model.fit(X, y, batch_size=10, epochs=20, validation_split=0.1, verbose=1, callbacks=[tensorboard])\n",
    "            \n",
    "#model.save('AAFT70LSTM.model')\n",
    "            \n",
    "print(\"Run the command line on anaconda prompt:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/logs/ --host localhost --port 8088 \" \\\n",
    "          \"\\nThen open http://localhost:8088/ into your web browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0214 18:42:53.936623 16368 deprecation.py:506] From C:\\Users\\admin\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0214 18:42:53.938618 16368 deprecation.py:506] From C:\\Users\\admin\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0214 18:42:53.939616 16368 deprecation.py:506] From C:\\Users\\admin\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0214 18:42:53.940613 16368 deprecation.py:506] From C:\\Users\\admin\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P01_TS_1.csv\n",
      "[1.05731466e-07 1.37668126e-03 9.98623252e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P01_TS_2.csv\n",
      "[2.5467313e-09 3.5447945e-05 9.9996459e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P02_TS_1.csv\n",
      "[2.1281483e-08 6.7037690e-05 9.9993289e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P02_TS_2.csv\n",
      "[2.4817875e-06 3.6132801e-02 9.6386468e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P02_TS_3.csv\n",
      "[1.4609939e-06 7.3261135e-03 9.9267238e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P04_TS_1.csv\n",
      "[4.0871328e-08 5.1931060e-05 9.9994802e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P04_TS_2.csv\n",
      "[4.5438227e-07 5.2760268e-05 9.9994683e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P04_TS_3.csv\n",
      "[1.1950857e-07 1.9935846e-04 9.9980050e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P05_TS_1.csv\n",
      "[4.9182969e-09 1.5361042e-04 9.9984634e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P05_TS_2.csv\n",
      "[1.6110872e-07 3.5807351e-04 9.9964178e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P05_TS_3.csv\n",
      "[5.791735e-09 9.426348e-05 9.999057e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P07_TS_1.csv\n",
      "[2.1873193e-05 9.2989326e-01 7.0084915e-02]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P07_TS_2.csv\n",
      "[3.8808292e-05 9.8377466e-01 1.6186578e-02]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P07_TS_3.csv\n",
      "[6.7775532e-06 9.9993432e-01 5.8858826e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P08_TS_1.csv\n",
      "[7.2663386e-07 2.7566475e-03 9.9724269e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P08_TS_2.csv\n",
      "[1.0915933e-05 1.3689910e-03 9.9862015e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P08_TS_3.csv\n",
      "[2.1587433e-05 4.1177403e-03 9.9586064e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P09_TS_1.csv\n",
      "[9.8484222e-08 9.9996185e-01 3.8076407e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P09_TS_2.csv\n",
      "[1.3966996e-07 9.9996054e-01 3.9323186e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P09_TS_3.csv\n",
      "[4.7059243e-07 9.9981648e-01 1.8307839e-04]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P10_TS_1.csv\n",
      "[4.00906572e-07 1.22041456e-04 9.99877572e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P10_TS_2.csv\n",
      "[5.6843842e-08 2.7879054e-05 9.9997211e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P10_TS_3.csv\n",
      "[2.0688534e-05 3.9723123e-04 9.9958211e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P12_TS_1.csv\n",
      "[8.1298850e-08 1.6549394e-04 9.9983442e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P12_TS_2.csv\n",
      "[3.9010299e-08 3.7319722e-04 9.9962676e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P12_TS_3.csv\n",
      "[7.1006305e-08 3.3386389e-04 9.9966609e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P14_TS_1.csv\n",
      "[7.2415949e-09 1.0763082e-04 9.9989235e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P14_TS_2.csv\n",
      "[2.1949327e-07 1.9877026e-04 9.9980098e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P16_TS_1.csv\n",
      "[1.2310363e-07 9.9992657e-01 7.3339907e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P16_TS_2.csv\n",
      "[5.9622337e-08 9.9999356e-01 6.3343509e-06]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P16_TS_3.csv\n",
      "[5.3585802e-07 9.9997449e-01 2.5065223e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P17_TS_1.csv\n",
      "[7.1359302e-08 6.2372140e-04 9.9937624e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P17_TS_2.csv\n",
      "[2.0696346e-08 1.2470401e-04 9.9987531e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P17_TS_3.csv\n",
      "[5.6300852e-08 1.7328816e-04 9.9982673e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P18_TS_1.csv\n",
      "[9.353484e-06 9.944781e-01 5.512526e-03]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P18_TS_2.csv\n",
      "[3.7951523e-07 9.9977618e-01 2.2352602e-04]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P18_TS_3.csv\n",
      "[5.180784e-06 9.977927e-01 2.202003e-03]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P19_TS_1.csv\n",
      "[1.8594158e-06 9.9997652e-01 2.1538859e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P19_TS_2.csv\n",
      "[1.1618139e-05 9.9996829e-01 2.0190311e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P19_TS_3.csv\n",
      "[6.2611868e-08 9.9999845e-01 1.4113349e-06]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P20_TS_1.csv\n",
      "[2.5299696e-06 1.0849842e-01 8.9149910e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P20_TS_2.csv\n",
      "[4.6174370e-07 9.8671848e-01 1.3281002e-02]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P20_TS_3.csv\n",
      "[2.0377913e-05 5.0483805e-01 4.9514154e-01]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P21_TS_1.csv\n",
      "[5.1231068e-09 1.3208833e-04 9.9986792e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P21_TS_2.csv\n",
      "[8.4115586e-09 1.7606141e-04 9.9982399e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P21_TS_3.csv\n",
      "[1.7333379e-08 3.9603058e-04 9.9960393e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P22_TS_1.csv\n",
      "[4.6304109e-07 2.3955657e-01 7.6044303e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P22_TS_2.csv\n",
      "[1.0589193e-06 9.5696712e-01 4.3031909e-02]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P23_TS_1.csv\n",
      "[8.7321359e-06 9.9981123e-01 1.8003929e-04]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P23_TS_2.csv\n",
      "[4.06470781e-06 9.99985933e-01 1.00502475e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P23_TS_3.csv\n",
      "[5.4563081e-04 9.5498234e-01 4.4472013e-02]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P24_TS_1.csv\n",
      "[9.994578e-01 5.061627e-04 3.615428e-05]\n",
      "Score 1\n",
      "\n",
      "\n",
      "P24_TS_2.csv\n",
      "[9.8620057e-01 4.2169538e-04 1.3377732e-02]\n",
      "Score 1\n",
      "\n",
      "\n",
      "P24_TS_3.csv\n",
      "[9.9349302e-01 2.6703655e-04 6.2399353e-03]\n",
      "Score 1\n",
      "\n",
      "\n",
      "P26_TS_1.csv\n",
      "[1.3286077e-05 6.3308157e-02 9.3667853e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P26_TS_2.csv\n",
      "[1.6042859e-05 1.6696742e-01 8.3301651e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P26_TS_3.csv\n",
      "[6.3021726e-06 9.8747718e-01 1.2516455e-02]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P27_TS_1.csv\n",
      "[9.9999833e-01 1.2747124e-06 3.7274870e-07]\n",
      "Score 1\n",
      "\n",
      "\n",
      "P27_TS_2.csv\n",
      "[9.9985766e-01 8.8340013e-05 5.4030010e-05]\n",
      "Score 1\n",
      "\n",
      "\n",
      "P27_TS_3.csv\n",
      "[9.9999869e-01 4.5675503e-07 8.3471053e-07]\n",
      "Score 1\n",
      "\n",
      "\n",
      "P28_TS_1.csv\n",
      "[3.6740123e-06 1.3051338e-02 9.8694497e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P28_TS_2.csv\n",
      "[1.0125029e-06 3.7920982e-02 9.6207798e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P28_TS_3.csv\n",
      "[7.380059e-07 9.939267e-01 6.072652e-03]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P30_TS_1.csv\n",
      "[2.0492430e-06 9.9995947e-01 3.8516951e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P30_TS_2.csv\n",
      "[1.2399190e-05 9.9997795e-01 9.6259100e-06]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P30_TS_3.csv\n",
      "[4.2747732e-05 5.6578654e-01 4.3417069e-01]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P32_TS_1.csv\n",
      "[1.6476935e-06 7.3695282e-04 9.9926144e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P32_TS_2.csv\n",
      "[1.5069133e-06 1.0291055e-03 9.9896944e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P32_TS_3.csv\n",
      "[3.8079961e-06 6.2591163e-04 9.9937028e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "P33_TS_1.csv\n",
      "[5.8462689e-07 9.9999917e-01 2.6261685e-07]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P33_TS_2.csv\n",
      "[1.8532858e-06 9.9999797e-01 1.0064159e-07]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P33_TS_3.csv\n",
      "[5.6877084e-08 9.9999976e-01 2.4773544e-07]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P34_TS_1.csv\n",
      "[1.3360625e-06 9.9998558e-01 1.3064396e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P34_TS_2.csv\n",
      "[6.586264e-05 9.999256e-01 8.430995e-06]\n",
      "Score 2\n",
      "\n",
      "\n",
      "P34_TS_3.csv\n",
      "[2.167960e-06 9.999927e-01 5.136475e-06]\n",
      "Score 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "path = r\"E:\\IDM downloads\\Documents\\Uni\\SIT University\\FYP\\My FYP\\New data\\Original\"\n",
    "CATEGORIES = [\"Score 1\", \"Score 2\", \"Score 3\"]\n",
    "\n",
    "def prepare(filepath):\n",
    "    sequences = []\n",
    "    traindata = pd.read_csv(filepath, header=None,names=['x', 'y', 'z'])\n",
    "    value = traindata.values\n",
    "    sequences.append((value))  # add this to our training_data\n",
    "    to_pad = 233\n",
    "    new_seq = []\n",
    "    for one_seq in sequences:\n",
    "        len_one_seq = len(one_seq)\n",
    "        last_val = one_seq[-1]\n",
    "        n = to_pad - len_one_seq\n",
    "   \n",
    "        to_concat = np.repeat(one_seq[-1], n).reshape(3, n).transpose()\n",
    "        new_one_seq = np.concatenate([one_seq, to_concat])\n",
    "        new_seq.append(new_one_seq)\n",
    "    X = np.stack(new_seq)\n",
    "    \n",
    "    #truncate the sequence to length xx\n",
    "    seq_len = 91\n",
    "    X=sequence.pad_sequences(X, maxlen=seq_len, padding='post', dtype='float', truncating='post')\n",
    "    return X *1.0/100 - 1.0 \n",
    "   \n",
    "    \n",
    "model = tf.keras.models.load_model(\"AAFT70LSTM.model\")\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    prediction = model.predict([prepare(os.path.join(path,filename))])\n",
    "    \n",
    "    print(filename)\n",
    "\n",
    "    print(prediction[0])\n",
    "\n",
    "    print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9997520e-01 2.0670081e-05 4.1715884e-06]\n",
      "Score 1\n",
      "\n",
      "\n",
      "[9.9997830e-01 1.4429069e-05 7.3298347e-06]\n",
      "Score 1\n",
      "\n",
      "\n",
      "[9.9981302e-01 1.8154636e-04 5.4757934e-06]\n",
      "Score 1\n",
      "\n",
      "\n",
      "[3.5620909e-05 9.9951649e-01 4.4784282e-04]\n",
      "Score 2\n",
      "\n",
      "\n",
      "[8.3453648e-05 9.9921632e-01 7.0011825e-04]\n",
      "Score 2\n",
      "\n",
      "\n",
      "[2.2757472e-06 9.9997723e-01 2.0492100e-05]\n",
      "Score 2\n",
      "\n",
      "\n",
      "[2.1013182e-05 9.1180584e-04 9.9906725e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "[3.3582005e-08 1.0028026e-04 9.9989963e-01]\n",
      "Score 3\n",
      "\n",
      "\n",
      "[1.5154270e-06 6.5876542e-05 9.9993265e-01]\n",
      "Score 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([prepare('P24_TS_1_001.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P24_TS_1_002.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P24_TS_1_003.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P07_TS_1_001.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P07_TS_1_002.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P07_TS_1_003.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P01_TS_1_001.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P01_TS_1_002.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "prediction = model.predict([prepare('P01_TS_1_003.csv')])\n",
    "\n",
    "print(prediction[0])\n",
    "\n",
    "print(CATEGORIES[int(np.argmax(prediction[0]))])\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
